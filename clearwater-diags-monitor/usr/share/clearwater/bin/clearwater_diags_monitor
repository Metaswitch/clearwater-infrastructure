#!/bin/bash

# @file clearwater_diags_monitor
#
# Copyright (C) Metaswitch Networks 2017
# If license terms are provided to you in a COPYING file in the root directory
# of the source code repository by which you are accessing this code, then
# the license outlined in that COPYING file applies to your use.
# Otherwise no rights are granted except for those provided to you by
# Metaswitch Networks in a separate written agreement.

# Redirect stdout and stderr to the logfile.
LOG_FILE=/var/log/clearwater-diags-monitor.log
exec 1>>$LOG_FILE
exec 2>>$LOG_FILE

DIAGS_DIR=/var/clearwater-diags-monitor
CRASH_DIR=$DIAGS_DIR/tmp
DUMPS_DIR=$DIAGS_DIR/dumps

# 1 gigabyte in kilobytes (assuming powers of 10 rather than powers of 2)
ONE_GB_IN_KB=1000000

. /etc/clearwater/config || exit

# Setup prefix to use when running commands that need to execute within
# the signaling network namespace for multi-interface configurations.
[ -z "$signaling_namespace" ] || namespace_prefix="ip netns exec $signaling_namespace"

# Setup dig's server argument for when it needs to execute within the
# signaling network namespace for multi-interface configurations.
[ -z "$signaling_dns_server" ] || namespace_dig_server="@$signaling_dns_server"

management_local_ip=${management_local_ip:-$local_ip}

log()
{
  printf "[$(date --utc +"%d-%h-%Y %H:%M:%S %Z")] $@\n"
}


# Wait until a specified file is closed.
# Params:
#  $1 - The file to wait for.
wait_until_closed()
{
  file=$1

  # Don't use inotifywait to wait for the file to close.  If it is already
  # closed we'll wait forever waiting for it to be closed /again/.
  while lsof $file >/dev/null 2>&1
  do
    sleep 1
  done
}


# Copy files and directories to the dump preserving the full path.
#
# For example the directory /var/log/sprout would be copied to
# <dumpdir>/var/log/sprout
#
# Params:
#   $1 - The objects to copy.
copy_to_dump()
{
  src=$(realpath $1)
  if [[ $src ]]
  then
    mkdir -p $CURRENT_DUMP_DIR/root
    cp -rp --parents $src $CURRENT_DUMP_DIR/root
  else
    log "$1 not present in deployment"
  fi
}

# Move files and directories to the dump preserving the full path.
# Use this when the file is very large or is used to trigger a diags
# collection (so we don't repeatedly trigger on the same file).
#
# For example the directory /var/log/sprout would be moved to
# <dumpdir>/var/log/sprout
#
# Params:
#   $1 - The objects to move.
move_to_dump()
{
  src=$(realpath $1)
  if [[ $src ]]
  then
    dst=$CURRENT_DUMP_DIR/root
    mkdir -p $dst

    # First ensure the directory structure is set up correctly
    for f in `find $src -type d`
    do
      mkdir -p $dst${f#$src}
    done

    # Now move the files
    for f in `find $src -type f`
    do
      mv $f $dst${f#$src}
    done
  else
    log "$1 not present in deployment"
  fi
}


# List all the clearwater packages installed.
clearwater_packages()
{
  {
    if which rpm > /dev/null ; then
      rpm -qa --qf '%{NAME} %{PACKAGER}\n'
    else
      dpkg-query -W -f='${Package} ${Maintainer}\n'
    fi
  } |
  grep " Project Clearwater Maintainers" |
  cut -d ' ' -f 1
}


# Get information about all the packages installed.
get_package_info()
{
  {
    if which rpm > /dev/null ; then
      rpm -qai
    else
      dpkg -s $(dpkg --get-selections | cut -f 1)
    fi
  } > $CURRENT_DUMP_DIR/package_info.txt
}

# Get configuration files for installed Clearwater packages
get_cw_package_config_files() {
  if which rpm > /dev/null ; then
    rpm -qc $CURRENT_CW_COMPONENTS
  else
    # The dpkg etc lines are of the form below, so we need the 2nd field from cut.
    #   <space><path-to-config-file><space><checksum>
    dpkg -s $CURRENT_CW_COMPONENTS | grep "/etc" | cut -d' ' -f 2
  fi
}

# Get information about all the clearwater packages installed.
get_cw_package_info()
{
  info_file=$CURRENT_DUMP_DIR/cw_package_info.txt

  cw_packages=$(clearwater_packages)
  {
    if [ "$cw_packages" ]
    then
      if which rpm > /dev/null ; then
        rpm -qi $cw_packages
      else
        dpkg -s $cw_packages
      fi
    else
      echo "No clearwater packages installed"
    fi
  } > $info_file
}


# Add a port to a domain string (if it doesn't already have one).
#
# For example:
#   add_default_port example.com 80       =>  example.com:80
#   add_default_port example.com:8888 80  =>  example.com:8888
#
# Params:
#   - $1 The domain string, which may include a port.
#   - $2 The default port.
add_default_port()
{
  domain=$1
  default_port=$2

  # The grep checks for:
  # -  Something that does not contain any colons followed by :<port> (which
  #    catches domains and IPv4 addresses with a port).
  # -  ']' followed by :<port> (which catches IPv6 addresses with a port).
  if ! echo $domain | grep -E '([^:]+|]):[0-9]+$'
  then
    domain="$domain:$default_port"
  fi

  echo $domain
}


# Split the domain and port in a domain-port combo.
# e.g. example.com:80 => example.com 80
#
# Params:
#  - $1 The domain/port combo to split.
split_domain_and_port()
{
  echo $1 | perl -ne '$_ =~ /(.+):([0-9]+)$/ && print "$1 $2\n"'
}


# Check that the server(s) specified by a domain name are contactable over the
# specified port.
#
# Params:
#   $1 - The domain.  May include a port.
#   $2 - Default port to use of the port is not specified as part of the domain.
check_connectivity_to_domain()
{
  raw_domain=$1
  default_port=$2

  if [ -z "$raw_domain" ]
  then
    log "No domain supplied"
    return 1
  fi

  # Puts the domain and port in the form <domain>:<port>
  domain_and_port=$(add_default_port $raw_domain $default_port)

  # Split the domain and port into separate variables.
  read -r domain port <<< "$(split_domain_and_port $domain_and_port)"

  if [ "$domain" = "0.0.0.0" ]
  then
    log "Not checking connectivity to 0.0.0.0"
    return 2
  fi

  file=$CURRENT_DUMP_DIR/connectivity_to_$domain.txt

  # First check we can resolve the domain name.
  $namespace_prefix dig $namespace_dig_server $domain >> $file

  # Now check that we can contact every server in the domain.
  echo "Check connectivity:" >> $file

  # List the servers by using dig again requesting just the answer section (the
  # addresses are in column 5).
  for ip in $($namespace_prefix dig $namespace_dig_server +noall +answer $domain | awk '{print $5}' )
  do
    if $namespace_prefix netcat -w1 $ip $port
    then
      echo "$ip:$port OK" >> $file
    else
      echo "$ip:$port FAILED" >> $file
    fi
  done
}


# Get the networking information for the system.
get_network_info()
{
  copy_to_dump "/etc/hosts"

  # For some reason, redirecting ifconfig to file doesn't work on CentOS, so we
  # pipe through tee instead.  We do the same below.
  ifconfig -a | tee $CURRENT_DUMP_DIR/ifconfig.txt > /dev/null
  netstat -rn > $CURRENT_DUMP_DIR/routes.txt
  netstat -anp > $CURRENT_DUMP_DIR/netstat.txt

  for ns in `ip netns list`
  do
    ip netns exec $ns ifconfig -a | tee $CURRENT_DUMP_DIR/ifconfig_$ns.txt > /dev/null
    ip netns exec $ns netstat -rn > $CURRENT_DUMP_DIR/routes_$ns.txt
    ip netns exec $ns netstat -anp > $CURRENT_DUMP_DIR/netstat_$ns.txt
  done
}


# Get the NTP setup for the system.
get_ntp_status()
{
  ntpq --numeric --peers > $CURRENT_DUMP_DIR/ntpq.txt
}


# Get the checksum files for all the clearwater packages installed on the
# system.
get_cw_package_checksums()
{
  # For Centos the --verify flag for the rpm command returns several
  # verification diags, including MD5 checksum problems (labelled with a `5`).
  # See https://linux.die.net/man/8/rpm for other flags description.
  {
    if which rpm > /dev/null ; then
      for pkg in $(clearwater_packages)
      do
        rpm --verify "$pkg"
      done
    fi
  } >> "$CURRENT_DUMP_DIR"/rpm_verification.txt

  # For Debian just collect the MD5 checksums
  if which dpkg > /dev/null ; then
    for pkg in $(clearwater_packages)
    do
      cp -p /var/lib/dpkg/info/"$pkg".md5sums "$CURRENT_DUMP_DIR"
    done
  fi
}


# Get informtion about the OS the system is running.
get_os_info()
{
  file=$CURRENT_DUMP_DIR/os.txt
  uname -a >> $file

  # For some reason lsb_release outputs "No LSB modules are available" to
  # stderr. Ignore this.
  lsb_release -a >> $file 2>/dev/null
}


# Get information about running processes.
get_process_info()
{
  ps -eaf > $CURRENT_DUMP_DIR/ps-eaf.txt
}


# Get information about the virtual hardware the system is running on.
get_hardware_info()
{
  lshw              > $CURRENT_DUMP_DIR/lshw.txt
  cat /proc/cpuinfo > $CURRENT_DUMP_DIR/cpuinfo.txt
  cat /proc/meminfo > $CURRENT_DUMP_DIR/meminfo.txt
  df -kh            > $CURRENT_DUMP_DIR/df-kh.txt
  fdisk -l          > $CURRENT_DUMP_DIR/fdisk-l.txt
}

# Get historical resource usage stats. This includes things like network usage,
# CPU usage, memory usge, etc.
get_usage_stats()
{
  # Use the sar tool to gather historical kernel statistics.  This arranges the
  # stats by day, so get yesterday's stats as well as today's (to get at least
  # a day's worth of stats even when collecting diags just after midnight).
  for day in today yesterday
  do
    # Use sar to record stats to a datestamped file. Options are:
    # -A : Get all stats
    # -f : Read stats from the specified file (where there is a file for each
    #      day of the month).
    sa_file=/var/log/sysstat/clearwater-sa$(date --utc +"%d" -d $day)
    if [ -e $sa_file ]
    then
      sar -A -f $sa_file > $CURRENT_DUMP_DIR/sar.$(date --utc "+%Y%m%d" -d $day).txt
    fi
  done
}

# Return whether any of the specified clearwater components are installed.
#
# For example `cw_component_installed bono sprout` will return 0 (true) if bono
# OR sprout are installed.
#
# Params:
#   $1 - The list of components to check.
cw_component_installed()
{
  for comp in "$@"
  do
    if echo $CURRENT_CW_COMPONENTS | tr ' ' '\n' | grep -q "^$comp$"
    then
      return 0
    fi
  done

  return 1
}


# Get information about the cassandra cluster.
get_cassandra_info()
{
  # Cassandra config.
  copy_to_dump "/etc/cassandra"

  # Get information about the Cassandra ring.
  $namespace_prefix nodetool status > $CURRENT_DUMP_DIR/nodetool_status.txt
  $namespace_prefix nodetool info > $CURRENT_DUMP_DIR/nodetool_info.txt
  $namespace_prefix nodetool cfstats > $CURRENT_DUMP_DIR/nodetool_cfstats.txt
  $namespace_prefix nodetool tpstats > $CURRENT_DUMP_DIR/nodetool_tpstats.txt
  $namespace_prefix nodetool netstats > $CURRENT_DUMP_DIR/nodetool_netstats.txt

  # Cassandra data format.
  echo "DESC SCHEMA;" | $namespace_prefix cqlsh > $CURRENT_DUMP_DIR/cassandra_schema.txt
  echo "DESC CLUSTER;" | $namespace_prefix cqlsh > $CURRENT_DUMP_DIR/cassandra_cluster.txt

  # Get the newest hprof file (make sure we move this file as it's likely big
  # and since it is used to trigger diags collections).
  hprof_files=$(ls -t /var/lib/cassandra/*hprof | head -1)
  for hprof_file in $hprof_files
  do
    wait_until_closed "$hprof_file"
    move_to_dump "$hprof_file"
  done
}


# Get information about the mysql database.
get_mysql_info()
{
  # Mysql config.
  copy_to_dump "/etc/mysql"

  # Server status and available databases.
  echo "show status" | mysql > $CURRENT_DUMP_DIR/mysql_show_status.txt
  echo "show databases" | mysql > $CURRENT_DUMP_DIR/mysql_show_databases.txt
}

# Get information about the etcd cluster.
get_etcd_info()
{
  clearwater-etcdctl member list > $CURRENT_DUMP_DIR/etcd_member_list.txt
  clearwater-etcdctl cluster-health > $CURRENT_DUMP_DIR/etcd_cluster_health.txt
  if [ -f /var/lib/clearwater-etcd/healthy_etcd_members ]
  then
    echo "When the cluster was last healthy, the members were" "$(</var/lib/clearwater-etcd/healthy_etcd_members)" >> $CURRENT_DUMP_DIR/etcd_cluster_health.txt
  else
    echo "The healthy etcd members file \"/var/lib/clearwater-etcd/healthy_etcd_members\" does not exist." >> $CURRENT_DUMP_DIR/etcd_cluster_health.txt
  fi
  curl "http://$management_local_ip:4000/v2/keys/clearwater?consistent=true&recursive=true&sorted=false" > $CURRENT_DUMP_DIR/etcd_state.txt
  curl "http://$management_local_ip:4000/v2/keys/clearwater?recursive=true&sorted=false" > $CURRENT_DUMP_DIR/etcd_state_no_consistency.txt
}

get_cluster_manager_info()
{
  /usr/share/clearwater/clearwater-cluster-manager/scripts/check_cluster_state > $CURRENT_DUMP_DIR/etcd_datastore_clusters_state.txt
}

get_config_manager_info()
{
  /usr/share/clearwater/clearwater-config-manager/scripts/check_config_sync > $CURRENT_DUMP_DIR/etcd_config_sync.txt
}

# Get information about the memcached database.
get_memcached_info()
{
  # Memcached config.
  copy_to_dump "/etc/memcached*"

  # Also get internal memcached stats (by sending the server a message saying
  # "stats").
  echo "stats" | $namespace_prefix netcat 127.0.0.1 11211 > $CURRENT_DUMP_DIR/memcached_stats.txt
}


# Return disk usage of a file or directory in kilobytes.
disk_usage_in_kb()
{
  du -ks $1 | cut -f 1
}

#
# Script starts here.
#
log "clearwater-diags-monitor starting"

if [ ! -z $signaling_namespace ] && [ $EUID -ne 0 ]
then
  echo "When using multiple networks, diags collection must be run as root"
  exit 2
fi

cd $CRASH_DIR

if [ $? -ne 0 ]; then
  echo "Crash directory didn't exist, creating"
  mkdir -p $CRASH_DIR
  chmod a+rwx $CRASH_DIR
  cd $CRASH_DIR
fi

while true
do

  # If the crash directory is empty wait for a new file to be created.
  if [ ! "$(ls -A)" ]
  then
    log "Waiting for trigger files"
    inotifywait -e create -qq .
  fi

  # Check to make sure we are at an acceptable level of disk usage, making sure
  # we're not using more than 1GB - we don't want to run out of disk space.
  # Pause for 20s before we start, just so that any previous process has an
  # opportunity to restart first.
  sleep 20
  while [ $(disk_usage_in_kb .) -gt $ONE_GB_IN_KB ]
  do
    trigger_to_delete=$(ls -At | head -n 1)
    log "Disk usage too high, deleting $trigger_to_delete"
    rm -rf $trigger_to_delete
    sleep 5
  done

  # Now we can start gathering.  First find the trigger files.
  trigger_files=$(ls -Atr)
  log "Processing trigger files: $(echo $trigger_files)"

  # Work out what component caused the dump.
  #
  # Triggers are of the form core.<cause>.<timestamp>.  For each trigger
  # extract the 2nd part of the filename and work out the distinct values.  Use
  # awk to do the field splitting as cut behaves oddly if the filename does not
  # match the expected pattern.
  #
  # Note the filenames are currently space separated, so use tr to put them each
  # on one line.
  triggers=$(echo $trigger_files | tr ' ' '\n' | awk 'BEGIN{FS="."} /^core/{print $2}' | sort | uniq)

  if [ -z "$triggers" ]
  then
    # No cause could be determined.  Maybe the file name is wrong?
    cause="unknown"
    log "Unknown trigger file(s): $(echo trigger_files)"
  elif [ $(echo $triggers | wc -w) -eq 1 ]
  then
    cause=$triggers
    log "Dump triggered by $cause"
  else
    cause="multiple"
    log "Dump has multiple causes"
  fi

  # Set up some variables that relate to the current dump.  These remain valid
  # for the duration of the dump collection process.
  BASE_DUMP=$(date --utc "+%Y%m%d%H%M%S")Z.$(hostname).$cause
  CURRENT_DUMP=$BASE_DUMP.temp
  CURRENT_DUMP_DIR=$DUMPS_DIR/$CURRENT_DUMP
  CURRENT_DUMP_ARCHIVE=$CURRENT_DUMP_DIR.tar.gz
  FINAL_DUMP_ARCHIVE=$DUMPS_DIR/$BASE_DUMP.tar.gz
  CURRENT_CW_COMPONENTS=$(clearwater_packages)

  # Create a new dump directory.
  mkdir $CURRENT_DUMP_DIR
  log "Gathering dump $CURRENT_DUMP"

  #
  # Now collect some diags
  #

  # Log files.
  copy_to_dump '/var/log'

  # PID files
  copy_to_dump '/var/run'

  # Config files
  copy_to_dump '/etc/clearwater'
  copy_to_dump '/etc/snmp'
  copy_to_dump '/etc/monit'
  copy_to_dump '/etc/hosts'

  # Get the config files for installed clearwater packages.
  for conffile in $(get_cw_package_config_files)
  do
    copy_to_dump $conffile
  done

  # Installed packages.
  get_cw_package_info
  get_cw_package_checksums
  get_package_info

  # Networking information.
  #
  # Connectivity between nodes is handled in per-node hooks as security groups
  # mean that not all nodes can contact all other nodes.
  get_network_info

  # NTP settings.
  get_ntp_status

  # Command histories.
  #
  # There is no history for root but commands run as sudo are logged in the auth
  # logs (/var/log/auth.log). We copy all of /var/log/ anyway.

  # Hardware information and historical resource usage.
  get_hardware_info
  get_usage_stats

  # OS and process info.
  get_os_info
  get_process_info

  # Database statuses.
  if cw_component_installed clearwater-cassandra
  then
    get_cassandra_info
  fi

  if cw_component_installed ellis
  then
    get_mysql_info
  fi

  if cw_component_installed memcached
  then
    get_memcached_info
  fi

  if cw_component_installed clearwater-etcd
  then
    get_etcd_info
  fi

  if cw_component_installed clearwater-cluster-manager
  then
    get_cluster_manager_info
  fi

  if cw_component_installed clearwater-config-manager
  then
    get_config_manager_info
  fi

  # Gather component specific diags for all installed components.
  #
  # Spin through the list of each diags script and execute it. Do this by
  # sourcing the script in (so it has access to all functions and environment
  # variables defined in this file) but also do it in a subshell (to prevent
  # the script from polluting our environment).
  scripts=$(find /usr/share/clearwater/clearwater-diags-monitor/scripts/ -maxdepth 1 -type f 2>/dev/null)
  log "Running extra diags scripts: $(echo $scripts)"

  for diags_script in $scripts
  do
    (
      # Give all the scripts their own subdirectory to write diags to (to stop
      # them from overwriting each other's diags).
      CURRENT_DUMP_DIR=$CURRENT_DUMP_DIR/$(basename $diags_script);
      mkdir $CURRENT_DUMP_DIR;

      . $diags_script
    )
  done

  # Copy this script to the dump file so we can tell what diags /should/ have
  # been collected.
  copy_to_dump $0

  # Wait for all the trigger files to have been written.
  for file in $trigger_files
  do
    wait_until_closed $file
    log "$file has been closed"
  done

  # Right, now we want to copy core files to the dump.  The algorithm is:
  # -  Always copy the oldest core.
  # -  Copy additional cores in decreasing age order, until the dump directory
  #    reaches 1GB in size.
  # -  Do not copy remaining dumps.
  #
  # This means that if there are multiple core files (e.g. if a process starts
  # cyclically crashing) we get the early cores from when the problem first
  # occured, but we don't waste disk space with lots of duplicate ones.
  trigger_files_arr=($trigger_files)

  core_file=${trigger_files_arr[0]}
  log "Moving $core_file to dump"
  mv $core_file $CURRENT_DUMP_DIR

  ii=1
  while [ $ii -lt ${#trigger_files_arr[@]} ]
  do
    core_file=${trigger_files_arr[$ii]}
    ii=$(( $ii + 1 ))

    # Get the size of the core file and dump directory in kB.
    core_file_size=$(disk_usage_in_kb $core_file)
    curr_dump_size=$(disk_usage_in_kb $CURRENT_DUMP_DIR)

    # If we have room, copy in the core file. Otherwise don't copy this file or
    # any more.
    if [ $(($core_file_size + $curr_dump_size)) -lt $ONE_GB_IN_KB ]
    then
      log "Moving $core_file to dump"
      mv $core_file $CURRENT_DUMP_DIR
    else
      log "No more space in dump"
      break
    fi
  done

  jj=0
  while [ $jj -lt $ii ]
  do
    core_file=${trigger_files_arr[$jj]}
    jj=$(( $jj + 1 ))

    # If the filename doesn't end in ".gz", gzip it
    if [[ "$core_file" != *.gz ]]; then
      log "Compressing $core_file"
      gzip $CURRENT_DUMP_DIR/$core_file
    fi
  done

  #
  # Diags have been collected.  Time to zip up the diags bundle.
  #

  # Finally we can compress the dump directory and delete it.
  #
  # We change to the dumps directory to do the tar as this removes the
  # directories above the current dump from the tar file.  Do all this in a
  # subshell so we can change directory freely.  The options to tar are:
  # -p  Preserve file permissions.
  # -c  Create an archive.
  # -z  Zip the archive.
  # -f  Name of the archive.
  (cd $DUMPS_DIR; tar -pcz -f $CURRENT_DUMP_ARCHIVE $CURRENT_DUMP)

  # Check whether the tar command succeeded
  ret_code=$?
  if [ $ret_code == 0 ]; then
    log "Diagnostic archive $CURRENT_DUMP_ARCHIVE created"
    rm -rf "$CURRENT_DUMP_DIR"
    mv $CURRENT_DUMP_ARCHIVE $FINAL_DUMP_ARCHIVE
  else
    # We failed to create the archive. The most common cause of this is lack of
    # disk space.
    # Given that the only files in the diags bundle which don't exist elsewhere
    # on the system are the core files, delete everything that's not a core file
    # in the temp directory, then just rename that to the archive name without
    # compressing it
    log "Failed to create diagnostic archive"

    # In a subshell:
    # - "set -e" so we don't delete files if we fail to cd to the dump dir
    # - cd to the dump directory
    # - get a list of all files/directories which don't begin with "core"
    # - delete them
    (set -e
     cd $CURRENT_DUMP_DIR
     non_core_files=$(ls | grep -v "^core*")
     for f in $non_core_files; do
       rm -rf $f
     done)

     # Now just rename the current dump dir to the archive name
     (cd $DUMPS_DIR; mv $CURRENT_DUMP_DIR $FINAL_DUMP_ARCHIVE)

     # Also delete the CURRENT_DUMP_ARCHIVE if it exists
     rm -f $CURRENT_DUMP_ARCHIVE
  fi

  # We should have dealt with all the trigger files by now, unless there are
  # more that we can deal with.  Delete any that are left over.
  rm -f $trigger_files

  # Delete old dumps until we're using less than 1GB of disk space.  du reports
  # in units of block size (kB).
  #
  # Take care not to delete the diagnostic dump we've just taken even if it
  # means exceeding the 1GB limit.
  while [ $(disk_usage_in_kb $DUMPS_DIR) -gt $ONE_GB_IN_KB ]
  do
    # Get oldest dump in the directory and delete it.
    #
    # If this is the dump we've just taken, it means we've deleted everything
    # in the directory except this dump and we're still over the limit.  In
    # this case, leave the latest diags set in place (even though this means
    # breaching the 1GB threshold).
    dump_to_delete=$(ls -t $DUMPS_DIR | tail -n 1)

    if [ "$DUMPS_DIR/$dump_to_delete" == "$FINAL_DUMP_ARCHIVE" ]
    then

      log "Diags dump just taken is $(du -ks $FINAL_DUMP_ARCHIVE | cut -f 1) KB. This will be preserved, despite the usual limit of 1GB on the /var/clearwater-diags-monitor/dumps directory"
      break
    fi

    log "Deleting dump $dump_to_delete"
    rm -rf "$DUMPS_DIR/$dump_to_delete"
  done
done
